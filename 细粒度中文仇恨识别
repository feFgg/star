import json
import re
import jieba
import numpy as np
from typing import List, Tuple, Dict, Any, Set
from collections import defaultdict, Counter
from sklearn.metrics import precision_recall_fscore_support
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
import difflib
import pickle
import os


class ChineseHateSpeechTrainer:

    def __init__(self):
        # 基础关键词词典（将从训练数据中扩展）
        self.hate_keywords = {
            "Racism": set(
                ["黑人", "白人", "黄种人", "混血", "外国人", "老外", "黑鬼", "白皮", "黑", "黑孩", "穷黑鬼", "小黑",
                 "小白"]),
            "Sexism": set(
                ["女司机", "母狗", "绿茶婊", "直男癌", "渣男", "花心女", "婊子", "荡妇", "女人", "男人", "女生",
                 "男生"]),
            "LGBTQ": set(["同性恋", "gay", "基佬", "拉拉", "变性人", "跨性别", "死基佬", "变态", "同志", "lgbt"]),
            "Religion": set(["穆斯林", "基督教", "佛教", "异教徒", "邪教", "迷信", "清真", "回民", "宗教"]),
            "Regionalism": set(["农村人", "外地人", "乡巴佬", "土包子", "穷山沟", "落后地区", "土鳖", "地域黑"]),
            "其他": set(["艾滋病", "残疾人", "精神病", "穷鬼", "富二代", "官二代", "弱智", "白痴", "傻子"])
        }

        self.hate_expressions = set([
            "去死", "该死", "垃圾", "废物", "滚", "恶心", "变态", "畜生",
            "不要脸", "丢人", "羞耻", "肮脏", "下贱", "卑劣", "讨厌", "烦人",
            "到处", "都是", "真是", "就是", "应该", "活该", "坏", "真的", "怎么",
            "傻逼", "智障", "脑残", "贱", "蠢", "愚蠢", "无知", "垃圾"
        ])

        # 高频组合词
        self.hate_combinations = [
            "真是恶心", "都是垃圾", "到处都是", "就是废物", "到处坏", "简直就是", "这些", "那些",
            "看不惯", "受不了", "滚出去", "都去死", "没素质", "不讲卫生", "就知道", "没教养"
        ]

        # 训练得到的模型和特征
        self.group_classifier = None
        self.hate_classifier = None
        self.vectorizer = None

        # 统计信息
        self.target_counter = Counter()
        self.argument_counter = Counter()
        self.group_counter = Counter()

        # 常见目标集合，根据实际训练数据扩充
        self.common_targets = set([
            "黑人", "白人", "女人", "男人", "同性恋", "穆斯林", "农村人", "外地人",
            "gay", "基佬", "变性人", "外国人", "女司机", "绿茶婊", "直男癌", "黑孩"
        ])

        jieba.initialize()
        print("训练器初始化完成")

    def parse_training_data(self, file_path: str) -> List[Dict]:
        """解析训练数据"""
        print(f"正在解析训练数据: {file_path}")
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
        except Exception as e:
            print(f"读取训练文件出错: {e}")
            try:
                with open(file_path, 'r', encoding='gbk') as f:
                    data = json.load(f)
                print("使用GBK编码成功读取文件")
            except Exception as e:
                print(f"所有编码尝试失败: {e}")
                return []

        parsed_data = []
        for i, item in enumerate(data):
            text = item.get('content', '')
            label = item.get('output', '')

            # 解析标注格式
            tuples = self._parse_label(label)

            # 更新常见目标集合
            for target, _, _, _ in tuples:
                if target != "NULL" and len(target) > 1:
                    self.common_targets.add(target)

            # 调试前几个样本
            if i < 5:
                print(f"\n样本 {i + 1}:")
                print(f"文本: {text}")
                print(f"标签: {label}")
                print(f"解析结果: {tuples}")

            parsed_data.append({
                'text': text,
                'tuples': tuples
            })

        print(f"解析了 {len(parsed_data)} 个训练样本")
        print(f"识别到的常见目标: {len(self.common_targets)} 个")
        return parsed_data

    def _parse_label(self, label: str) -> List[Tuple[str, str, str, str]]:
        """解析标注格式为四元组"""
        if not label or label.strip() == "":
            return [("NULL", "NULL", "其他", "Non-hate")]

        # 移除[END]标记
        label = label.replace(" [END]", "").strip()

        # 处理空字符串情况
        if not label:
            return [("NULL", "NULL", "其他", "Non-hate")]

        # 按[SEP]分割多个四元组
        tuple_strings = label.split(" [SEP] ")

        tuples = []
        for tuple_str in tuple_strings:
            if not tuple_str.strip():
                continue

            # 处理格式问题：确保使用 | 分隔符
            if " | " not in tuple_str and "|" in tuple_str:
                tuple_str = tuple_str.replace("|", " | ")

            parts = [part.strip() for part in tuple_str.split(" | ")]

            # 确保有足够的部分
            if len(parts) == 4:
                # 规范化hate/non-hate标签
                parts[3] = self._normalize_hate_label(parts[3])
                tuples.append(tuple(parts))
            elif len(parts) == 1 and not parts[0]:
                continue
            else:
                # 尝试修复不完整的四元组
                if 1 <= len(parts) < 4:
                    while len(parts) < 4:
                        if len(parts) == 1:
                            parts.append("NULL")
                        elif len(parts) == 2:
                            parts.append("其他")
                        elif len(parts) == 3:
                            parts.append("Non-hate")
                    # 规范化hate/non-hate标签
                    parts[3] = self._normalize_hate_label(parts[3])
                    tuples.append(tuple(parts))

        # 如果解析后仍然没有有效四元组，返回默认四元组
        if not tuples:
            return [("NULL", "NULL", "其他", "Non-hate")]

        return tuples

    def _normalize_hate_label(self, label: str) -> str:
        """规范化hate/non-hate标签"""
        label = label.lower().strip()
        if label in ["hate", "hatred", "hateful", "仇恨", "恨", "hate speech"]:
            return "hate"
        return "Non-hate"

    def extract_features_from_training(self, training_data: List[Dict]):
        """从训练数据中提取特征和模式"""
        print("开始从训练数据中提取特征...")
        all_texts = []
        all_groups = []
        all_hate_labels = []

        for item in training_data:
            text = item['text']
            tuples = item['tuples']

            if not tuples:
                continue

            for target, argument, group, hate_label in tuples:
                all_texts.append(text)
                all_groups.append(group)
                all_hate_labels.append(hate_label)

                # 更新计数器
                self.target_counter[target] += 1
                self.argument_counter[argument] += 1
                self.group_counter[group] += 1

                # 从标注数据中提取关键词
                self._extract_keywords_from_sample(text, target, argument, group, hate_label)

        print(f"提取了 {len(all_texts)} 个有效的训练样本")
        print(f"目标计数器: {dict(self.target_counter)}")
        print(f"群体计数器: {dict(self.group_counter)}")
        print(f"仇恨关键词数量: {sum(len(words) for words in self.hate_keywords.values())}")
        print(f"仇恨表达数量: {len(self.hate_expressions)}")

        # 训练分类器
        if all_texts:
            self._train_classifiers(all_texts, all_groups, all_hate_labels)
        else:
            print("警告: 没有有效的训练样本，分类器未训练")

        print(f"训练完成！群体分布: {dict(self.group_counter)}")

    def _extract_keywords_from_sample(self, text: str, target: str, argument: str, group: str, hate_label: str):
        """从单个样本中提取关键词"""
        words = list(jieba.cut(text))

        # 确保群体类别存在
        if group not in self.hate_keywords:
            self.hate_keywords[group] = set()

        # 提取群体相关关键词
        for word in words:
            if len(word) > 1 and word in text:
                # 目标相关词
                if target in word or word in target:
                    self.hate_keywords[group].add(word)

                # 群体相关词
                if group == "Racism" and any(kw in word for kw in ["人种", "黑", "白", "黄", "种族"]):
                    self.hate_keywords[group].add(word)
                elif group == "Sexism" and any(kw in word for kw in ["女", "男", "性别", "妇女"]):
                    self.hate_keywords[group].add(word)
                elif group == "LGBTQ" and any(kw in word for kw in ["同性", "性向", "取向", "gay"]):
                    self.hate_keywords[group].add(word)
                elif group == "Religion" and any(kw in word for kw in ["教", "信仰", "穆斯林", "清真"]):
                    self.hate_keywords[group].add(word)
                elif group == "Regionalism" and any(kw in word for kw in ["地方", "地区", "乡", "城", "市", "省"]):
                    self.hate_keywords[group].add(word)

        # 提取仇恨表达
        if hate_label.lower() == "hate":
            for word in words:
                if len(word) > 1 and word not in ["的", "了", "和", "在", "是", "有", "就", "我", "你", "他", "她",
                                                  "它"]:
                    if word in argument:
                        self.hate_expressions.add(word)

            # 检查是否有组合表达
            for combo in self.hate_combinations:
                if combo in text:
                    parts = combo.split()
                    for part in parts:
                        if len(part) > 1:
                            self.hate_expressions.add(part)

    def _train_classifiers(self, texts: List[str], groups: List[str], hate_labels: List[str]):
        """训练分类器"""
        print("开始训练分类器...")
        # 使用TF-IDF向量化文本
        self.vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 3))
        X = self.vectorizer.fit_transform(texts)

        # 输出一些特征信息
        print(f"TF-IDF特征维度: {X.shape}")
        feature_names = self.vectorizer.get_feature_names_out()
        print(f"前10个特征: {feature_names[:10]}")

        # 训练群体分类器 - 使用更健壮的随机森林分类器
        self.group_classifier = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')
        self.group_classifier.fit(X, groups)

        # 训练仇恨分类器 - 使用逻辑回归
        self.hate_classifier = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced')
        hate_binary = [1 if label.lower() == "hate" else 0 for label in hate_labels]
        self.hate_classifier.fit(X, hate_binary)

        print(f"分类器训练完成，特征维度: {X.shape[1]}")
        print(f"群体类别数量: {len(set(groups))}")
        print(f"仇恨样本比例: {sum(hate_binary) / len(hate_binary):.2f}")

    def save_model(self, model_path: str):
        """保存训练好的模型"""
        print(f"正在保存模型到: {model_path}")
        model_data = {
            'hate_keywords': {k: list(v) for k, v in self.hate_keywords.items()},
            'hate_expressions': list(self.hate_expressions),
            'hate_combinations': self.hate_combinations,
            'target_counter': dict(self.target_counter),
            'argument_counter': dict(self.argument_counter),
            'group_counter': dict(self.group_counter),
            'common_targets': list(self.common_targets),
            'vectorizer': self.vectorizer,
            'group_classifier': self.group_classifier,
            'hate_classifier': self.hate_classifier
        }

        try:
            with open(model_path, 'wb') as f:
                pickle.dump(model_data, f)
            print(f"模型已成功保存到: {model_path}")
        except Exception as e:
            print(f"保存模型失败: {e}")


class ChineseHateSpeechDetector:
    """中文仇恨言论检测器"""

    def __init__(self, model_path: str = None):
        # 默认配置
        self.hate_keywords = {
            "Racism": set(["黑人", "白人", "黄种人", "混血", "外国人", "老外", "黑", "黑孩"]),
            "Sexism": set(["女司机", "母狗", "绿茶婊", "直男癌", "女人", "男人"]),
            "LGBTQ": set(["同性恋", "gay", "基佬", "拉拉", "变性人"]),
            "Religion": set(["穆斯林", "基督教", "佛教", "宗教"]),
            "Regionalism": set(["农村人", "外地人", "乡巴佬", "土包子"]),
            "其他": set(["艾滋病", "残疾人", "精神病", "穷鬼"])
        }

        self.hate_expressions = set([
            "去死", "该死", "垃圾", "废物", "滚", "恶心", "变态", "畜生",
            "到处", "都是", "真是", "就是", "坏"
        ])

        self.hate_combinations = [
            "真是恶心", "都是垃圾", "到处都是", "就是废物", "到处坏"
        ]

        self.common_targets = set([
            "黑人", "白人", "女人", "男人", "同性恋", "穆斯林", "农村人", "外地人", "黑孩"
        ])

        self.vectorizer = None
        self.group_classifier = None
        self.hate_classifier = None

        # 如果提供了模型路径，加载训练好的模型
        if model_path and os.path.exists(model_path):
            self.load_model(model_path)
        else:
            print(f"警告: 模型路径 {model_path} 不存在或未提供，使用默认配置")

        jieba.initialize()
        print("检测器初始化完成")

    def load_model(self, model_path: str):
        """加载训练好的模型"""
        print(f"正在加载模型: {model_path}")
        try:
            with open(model_path, 'rb') as f:
                model_data = pickle.load(f)

            self.hate_keywords = {k: set(v) for k, v in model_data['hate_keywords'].items()}
            self.hate_expressions = set(model_data['hate_expressions'])

            # 加载新增属性，兼容旧模型
            if 'hate_combinations' in model_data:
                self.hate_combinations = model_data['hate_combinations']
            if 'common_targets' in model_data:
                self.common_targets = set(model_data['common_targets'])

            self.vectorizer = model_data['vectorizer']
            self.group_classifier = model_data['group_classifier']
            self.hate_classifier = model_data['hate_classifier']

            print(f"模型已从 {model_path} 加载")
            print(f"加载的关键词数量: {sum(len(words) for words in self.hate_keywords.values())}")
            print(f"加载的仇恨表达数量: {len(self.hate_expressions)}")
            print(f"加载的常见目标数量: {len(self.common_targets)}")
        except Exception as e:
            print(f"加载模型失败: {e}，使用默认配置")

    def extract_target(self, text: str) -> str:
        """提取评论对象 - 增强版"""
        # 1. 首先尝试在常见目标集合中匹配
        for target in sorted(self.common_targets, key=len, reverse=True):
            if target in text:
                return target

        # 2. 从关键词词典中匹配
        for group_keywords in self.hate_keywords.values():
            for keyword in sorted(group_keywords, key=len, reverse=True):
                if keyword in text and len(keyword) > 1:
                    return keyword

        # 3. 使用模式匹配
        target_patterns = [
            r'(那些[\w\u4e00-\u9fff]{1,8})',
            r'(这些[\w\u4e00-\u9fff]{1,8})',
            r'([\w\u4e00-\u9fff]{1,8}群体)',
            r'([\w\u4e00-\u9fff]{1,8}人)',
            r'([\w\u4e00-\u9fff]{1,8}佬)',
            r'([\w\u4e00-\u9fff]{1,8}鬼)',
            r'([\w\u4e00-\u9fff]{1,5}族)',
            r'([\w\u4e00-\u9fff]{1,5}性恋)'
        ]

        for pattern in target_patterns:
            match = re.search(pattern, text)
            if match:
                return match.group(1)

        # 4. 使用分词找到可能的实体
        words = list(jieba.cut(text))
        for word in words:
            if len(word) > 1 and word not in self.hate_expressions:
                # 检查是否可能是目标词
                if any(char in word for char in ["人", "族", "佬", "鬼", "民", "性"]):
                    return word

        # 5. 如果都没有找到，尝试使用文本的前几个有意义的词
        for word in words:
            if len(word) > 1 and word not in ["的", "了", "和", "在", "是", "有", "就", "我", "你", "他", "她", "它"]:
                return word

        # 6. 最后，如果都没有找到，尝试直接使用文本的前几个字符
        if len(text) > 0:
            return text[:min(5, len(text))]

        return "NULL"

    def extract_argument(self, text: str, target: str) -> str:
        """提取论点信息片段"""
        # 首先检查是否有组合表达
        for combo in sorted(self.hate_combinations, key=len, reverse=True):
            if combo in text:
                return combo

        # 如果目标不是NULL，提取包含目标和仇恨表达的片段
        if target != "NULL" and target in text:
            target_pos = text.find(target)

            # 查找目标附近的仇恨表达
            best_expr = None
            best_dist = float('inf')

            for hate_expr in sorted(self.hate_expressions, key=len, reverse=True):
                if hate_expr in text:
                    hate_pos = text.find(hate_expr)
                    dist = abs(target_pos - hate_pos)

                    # 找到最接近目标的表达
                    if dist < best_dist:
                        best_dist = dist
                        best_expr = hate_expr

            if best_expr:
                hate_pos = text.find(best_expr)
                # 提取目标和仇恨表达之间的片段
                start = min(target_pos, hate_pos)
                end = max(target_pos + len(target), hate_pos + len(best_expr))

                # 如果片段太短，扩展它
                if end - start < 10:
                    start = max(0, start - 5)
                    end = min(len(text), end + 5)

                return text[start:end]

            # 如果没有找到仇恨表达，返回目标周围的文本
            start = max(0, target_pos - 8)
            end = min(len(text), target_pos + len(target) + 12)
            return text[start:end]

        # 查找包含仇恨表达的句子片段
        words = list(jieba.cut(text))
        for i, word in enumerate(words):
            if word in self.hate_expressions:
                start = max(0, i - 4)
                end = min(len(words), i + 5)
                return ''.join(words[start:end])

        # 如果实在找不到，检查一些常见的负面短语
        negative_phrases = ["太差", "真烦", "好烦", "讨厌", "不喜欢", "受不了", "烦死了"]
        for phrase in negative_phrases:
            if phrase in text:
                pos = text.find(phrase)
                start = max(0, pos - 5)
                end = min(len(text), pos + len(phrase) + 5)
                return text[start:end]

        # 如果实在找不到，直接返回整个文本（可能需要截取）
        return text[:min(30, len(text))]

    def identify_targeted_group(self, text: str, target: str = None) -> str:
        """识别目标群体 - 增强版"""
        # 如果有训练好的分类器，使用分类器
        if self.group_classifier and self.vectorizer:
            try:
                X = self.vectorizer.transform([text])
                predicted_group = self.group_classifier.predict(X)[0]
                return predicted_group
            except Exception as e:
                print(f"分类器预测失败: {e}")

        # 如果有明确的目标，先根据目标判断
        if target and target != "NULL":
            # 检查目标是否在任何关键词集合中
            for group, keywords in self.hate_keywords.items():
                if target in keywords or any(target in keyword or keyword in target for keyword in keywords):
                    return group

            # 基于目标的特征词匹配
            if any(word in target for word in ["黑", "白", "黄", "种族", "人种"]):
                return "Racism"
            elif any(word in target for word in ["女", "男", "性别"]):
                return "Sexism"
            elif any(word in target for word in ["同性恋", "gay", "les", "基佬", "同志", "性向"]):
                return "LGBTQ"
            elif any(word in target for word in ["穆斯林", "教徒", "信徒", "教", "宗教"]):
                return "Religion"
            elif any(word in target for word in ["农村", "城市", "地区", "地方", "乡", "土"]):
                return "Regionalism"

        # 使用关键词匹配
        for group, keywords in self.hate_keywords.items():
            for keyword in keywords:
                if keyword in text:
                    return group

        # 基于文本内容的简单规则
        if any(word in text for word in ["黑", "白", "黄", "种族", "民族", "肤色", "人种"]):
            return "Racism"
        elif any(word in text for word in ["女", "男", "性别", "妇女", "先生", "太太", "女士"]):
            return "Sexism"
        elif any(word in text for word in ["同性恋", "gay", "les", "基佬", "性取向", "lgbt"]):
            return "LGBTQ"
        elif any(word in text for word in ["穆斯林", "基督", "佛教", "宗教", "教徒", "信仰"]):
            return "Religion"
        elif any(word in text for word in ["农村", "外地", "地区", "省", "市", "县", "乡"]):
            return "Regionalism"

        return "其他"

    def is_hateful(self, text: str, argument: str, target: str = None) -> str:
        """判断是否为仇恨言论"""
        # 如果有训练好的分类器，使用分类器
        if self.hate_classifier and self.vectorizer:
            try:
                X = self.vectorizer.transform([text])
                prediction = self.hate_classifier.predict(X)[0]

                # 高置信度直接返回
                proba = self.hate_classifier.predict_proba(X)[0]
                if max(proba) > 0.75:
                    return "hate" if prediction == 1 else "Non-hate"

                # 低置信度场景，结合规则判断
            except Exception as e:
                print(f"仇恨分类器预测失败: {e}")

        # 使用规则方法
        hate_score = 0

        # 检查仇恨表达
        for hate_word in sorted(self.hate_expressions, key=len, reverse=True):
            if hate_word in text or hate_word in argument:
                hate_score += 1
                # 特别恶劣的词汇加权
                if hate_word in ["恶心", "垃圾", "废物", "去死", "该死", "滚", "畜生"]:
                    hate_score += 1

        # 检查组合表达
        for combo in self.hate_combinations:
            if combo in text or combo in argument:
                hate_score += 2

        # 检查否定词汇
        negative_words = ["恶心", "垃圾", "废物", "去死", "该死", "变态", "畜生", "滚", "坏",
                          "傻逼", "智障", "脑残", "白痴", "愚蠢", "贱", "蠢", "厌恶"]
        for word in negative_words:
            if word in text:
                hate_score += 2

        # 检查上下文组合
        if target and target != "NULL":
            target_context = [f"这些{target}", f"那些{target}", f"{target}都", f"讨厌{target}",
                              f"{target}真", f"真是{target}", f"{target}去死", f"不喜欢{target}"]
            for context in target_context:
                if context in text:
                    hate_score += 2

        # 检查贬义词组合
        if any(combo in text for combo in ["真是恶心", "都是垃圾", "到处都是", "就是废物", "到处坏",
                                           "简直就是", "都去死", "没素质", "不讲卫生"]):
            hate_score += 3

        # 特殊判断短文本
        if len(text) < 10 and any(word in text for word in negative_words):
            hate_score += 2

        # 根据目标的存在来调整分数
        if target and target != "NULL" and hate_score >= 1:
            hate_score += 1

        return "hate" if hate_score >= 3 else "Non-hate"

    def extract_tuples(self, text: str) -> List[Tuple[str, str, str, str]]:
        """从文本中提取仇恨四元组"""
        if not text or len(text.strip()) < 3:
            return [("NULL", "NULL", "其他", "Non-hate")]

        # 1. 提取目标
        target = self.extract_target(text)

        # 2. 提取论点
        argument = self.extract_argument(text, target)

        # 3. 识别目标群体
        targeted_group = self.identify_targeted_group(text, target)

        # 4. 判断是否仇恨
        hateful = self.is_hateful(text, argument, target)

        # 5. 执行后处理规则

        # 规则1: 如果目标是"NULL"但论点明确，尝试从论点中提取目标
        if target == "NULL" and argument != text[:min(30, len(text))]:
            possible_target = self.extract_target(argument)
            if possible_target != "NULL" and possible_target != argument:
                target = possible_target
                # 重新判断群体
                targeted_group = self.identify_targeted_group(text, target)

        # 规则2: 如果是仇恨言论但没有明确目标，设置为NULL
        if hateful == "hate" and target == "NULL":
            # 尝试再次寻找目标
            for group, keywords in self.hate_keywords.items():
                for keyword in keywords:
                    if keyword in text:
                        target = keyword
                        targeted_group = group
                        break
                if target != "NULL":
                    break

        # 规则3: 非仇恨言论的简化处理
        if hateful == "Non-hate" and (target == "NULL" or argument == text[:min(30, len(text))]):
            return [("NULL", "NULL", "其他", "Non-hate")]

        # 规则4: 根据测试集数据特点，调整输出格式
        # 如果目标出现在论点中，从论点中提取目标周围的文本
        if target != "NULL" and target in argument:
            arg_start = argument.find(target)
            start_idx = max(0, arg_start - 2)
            end_idx = min(len(argument), arg_start + len(target) + 5)
            argument = argument[start_idx:end_idx]

        # 返回单个四元组
        return [(target, argument, targeted_group, hateful)]

    def format_output(self, tuples: List[Tuple[str, str, str, str]]) -> str:
        """格式化输出结果"""
        if not tuples:
            return "NULL | NULL | 其他 | Non-hate [END]"

        formatted_tuples = []
        for target, argument, group, hate_label in tuples:
            # 严格规范化每个元素
            if not target or target.strip() == "":
                target = "NULL"

            if not argument or argument.strip() == "":
                argument = "NULL"

            # 群体标签规范化 - 严格按照标准格式
            if not group or group.strip() == "":
                group = "Others"
            elif group.lower() in ["racism", "racist", "种族"]:
                group = "Racism"
            elif group.lower() in ["sexism", "sexist", "性别"]:
                group = "Sexism"
            elif group.lower() in ["lgbtq", "lgbt", "同性恋"]:
                group = "LGBTQ"
            elif group.lower() in ["regionalism", "region", "地域"]:
                group = "Region"
            else:
                group = "Others"

            # hate标签规范化 - 严格按照标准格式
            if hate_label.lower() in ["hate", "hatred", "hateful", "仇恨"]:
                hate_label = "hate"
            else:
                hate_label = "Non-hate"

            formatted_tuple = f"{target} | {argument} | {group} | {hate_label}"
            formatted_tuples.append(formatted_tuple)

        if len(formatted_tuples) > 1:
            return " [SEP] ".join(formatted_tuples) + " [END]"
        else:
            return formatted_tuples[0] + " [END]"


class HateSpeechEvaluator:
    """仇恨言论评价器"""

    def parse_output(self, output: str) -> List[Tuple[str, str, str, str]]:
        """解析输出格式为四元组列表"""
        if not output or output.strip() == "":
            return [("NULL", "NULL", "其他", "Non-hate")]

        # 移除[END]标记
        output = output.replace(" [END]", "").strip()

        # 处理空字符串情况
        if not output:
            return [("NULL", "NULL", "其他", "Non-hate")]

        tuple_strings = output.split(" [SEP] ")

        tuples = []
        for tuple_str in tuple_strings:
            if not tuple_str.strip():
                continue

            # 处理格式问题：确保使用 | 分隔符
            if " | " not in tuple_str and "|" in tuple_str:
                tuple_str = tuple_str.replace("|", " | ")

            parts = [part.strip() for part in tuple_str.split(" | ")]

            # 确保有足够的部分
            if len(parts) == 4:
                # 规范化hate/Non-hate标签
                parts[3] = self._normalize_hate_label(parts[3])
                tuples.append(tuple(parts))
            elif len(parts) == 1 and not parts[0]:
                continue
            else:
                # 尝试修复不完整的四元组
                if 1 <= len(parts) < 4:
                    while len(parts) < 4:
                        if len(parts) == 1:
                            parts.append("NULL")
                        elif len(parts) == 2:
                            parts.append("其他")
                        elif len(parts) == 3:
                            parts.append("Non-hate")
                    # 规范化hate/Non-hate标签
                    parts[3] = self._normalize_hate_label(parts[3])
                    tuples.append(tuple(parts))

        # 如果解析后仍然没有有效四元组，返回默认四元组
        if not tuples:
            return [("NULL", "NULL", "其他", "Non-hate")]

        return tuples

    def _normalize_hate_label(self, label: str) -> str:
        """规范化hate/non-hate标签"""
        label = label.lower().strip()
        if label in ["hate", "hatred", "hateful", "仇恨", "恨", "hate speech"]:
            return "hate"
        return "Non-hate"

    def normalize_text(self, text: str) -> str:
        """标准化文本，删除多余空格和标点"""
        text = re.sub(r'\s+', ' ', text).strip()
        return text.lower()

    def hard_match(self, pred_tuples: List[Tuple], true_tuples: List[Tuple]) -> Tuple[float, float, float]:
        """硬匹配评估"""
        # 处理空元组情况
        if not pred_tuples and not true_tuples:
            return 1.0, 1.0, 1.0
        elif not pred_tuples or not true_tuples:
            return 0.0, 0.0, 0.0

        # 转换为集合前标准化文本
        pred_set = set()
        true_set = set()

        for t in pred_tuples:
            normalized = tuple(self.normalize_text(part) for part in t)
            pred_set.add(normalized)

        for t in true_tuples:
            normalized = tuple(self.normalize_text(part) for part in t)
            true_set.add(normalized)

        # 精确匹配
        exact_matches = pred_set & true_set

        # 检查部分匹配（只匹配target和hate_label）
        partial_matches = set()
        if not exact_matches:
            for pred in pred_set:
                for true in true_set:
                    # 检查目标和是否仇恨的匹配
                    if pred[0] == true[0] and pred[3] == true[3]:
                        # 相同的目标和是否仇恨，计为部分匹配
                        partial_matches.add((pred, true))

        # 计算匹配数
        tp = len(exact_matches) + len(partial_matches) * 0.5
        fp = len(pred_set) - len(exact_matches) - len(partial_matches) * 0.5
        fn = len(true_set) - len(exact_matches) - len(partial_matches) * 0.5

        # 调试输出
        if exact_matches:
            print(f"硬匹配成功! 精确匹配: {exact_matches}")
        if partial_matches:
            print(f"部分匹配! 数量: {len(partial_matches)}")

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return precision, recall, f1

    def soft_match(self, pred_tuples: List[Tuple], true_tuples: List[Tuple], threshold: float = 0.3) -> Tuple[
        float, float, float]:
        """软匹配评估"""
        # 如果预测或真实标签为空，特殊处理
        if not pred_tuples and not true_tuples:
            return 1.0, 1.0, 1.0
        elif not pred_tuples or not true_tuples:
            return 0.0, 0.0, 0.0

        def tuple_similarity(t1: Tuple, t2: Tuple) -> float:
            """计算两个四元组的相似度，增强版"""
            if len(t1) != 4 or len(t2) != 4:
                return 0.0

            # 标准化文本
            t1 = tuple(self.normalize_text(str(item)) for item in t1)
            t2 = tuple(self.normalize_text(str(item)) for item in t2)

            # 给每个字段分配权重
            weights = [0.35, 0.15, 0.15, 0.35]  # target, argument, group, hate_label

            weighted_sim = 0.0
            for i in range(len(t1)):
                # 处理NULL情况
                if (t1[i] == "null" and t2[i] == "null") or (t1[i] == "空" and t2[i] == "空"):
                    sim = 1.0
                # 对于hate_label字段，严格匹配
                elif i == 3:
                    if t1[i] == t2[i]:
                        sim = 1.0
                    else:
                        sim = 0.0
                # 对于群体字段，更宽松匹配
                elif i == 2:
                    if t1[i] == t2[i]:
                        sim = 1.0
                    elif t1[i] == "其他" or t2[i] == "其他":
                        sim = 0.7
                    else:
                        # 同为仇恨类别给一定相似度
                        non_other_groups = ["racism", "sexism", "lgbtq", "religion", "regionalism"]
                        if t1[i] in non_other_groups and t2[i] in non_other_groups:
                            sim = 0.3
                        else:
                            sim = 0.0
                # 对于target字段，检查包含关系
                elif i == 0:
                    if t1[i] == t2[i]:
                        sim = 1.0
                    elif t1[i] == "null" or t2[i] == "null":
                        sim = 0.3
                    elif t1[i] in t2[i] or t2[i] in t1[i]:
                        sim = 0.8
                    else:
                        sim = difflib.SequenceMatcher(None, t1[i], t2[i]).ratio()
                        if sim > 0.7:
                            sim = sim * 1.2
                # 对于argument字段，更宽松的匹配
                else:
                    if t1[i] == "null" or t2[i] == "null":
                        sim = 0.5
                    elif t1[i] in t2[i] or t2[i] in t1[i]:
                        length_ratio = min(len(t1[i]), len(t2[i])) / max(len(t1[i]), len(t2[i]))
                        sim = 0.7 + 0.3 * length_ratio
                    else:
                        sim = difflib.SequenceMatcher(None, t1[i], t2[i]).ratio()
                        sim = min(1.0, sim * 1.2)

                weighted_sim += weights[i] * sim

            # 特殊规则：如果目标和hate完全匹配，给予额外奖励
            if t1[0] == t2[0] and t1[3] == t2[3]:
                weighted_sim = min(1.0, weighted_sim + 0.1)

            return weighted_sim

        # 记录每个预测与每个真实标签的最佳匹配
        matched_pred = set()
        matched_true = set()

        # 先尝试完全匹配
        for i, pred_tuple in enumerate(pred_tuples):
            for j, true_tuple in enumerate(true_tuples):
                if j not in matched_true:
                    if tuple(self.normalize_text(str(item)) for item in pred_tuple) == tuple(
                            self.normalize_text(str(item)) for item in true_tuple):
                        matched_pred.add(i)
                        matched_true.add(j)
                        print(f"完全匹配成功: {pred_tuple} == {true_tuple}")
                        break

        # 然后进行相似度匹配
        for i, pred_tuple in enumerate(pred_tuples):
            if i in matched_pred:
                continue

            best_match = -1
            best_sim = 0

            for j, true_tuple in enumerate(true_tuples):
                if j not in matched_true:
                    similarity = tuple_similarity(pred_tuple, true_tuple)

                    if similarity > best_sim and similarity >= threshold:
                        best_sim = similarity
                        best_match = j

            if best_match != -1:
                matched_pred.add(i)
                matched_true.add(best_match)
                print(f"相似度匹配成功: {pred_tuple} -> {true_tuples[best_match]} (相似度: {best_sim:.2f})")

        tp = len(matched_pred)
        fp = len(pred_tuples) - tp
        fn = len(true_tuples) - len(matched_true)

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0

        return precision, recall, f1

    def evaluate(self, predictions: List[str], ground_truths: List[str]) -> Dict[str, float]:
        """评估预测结果"""
        print(f"预测结果总数: {len(predictions)}")
        print(f"真实标签总数: {len(ground_truths)}")

        # 打印几个样本比较
        for i in range(min(5, len(predictions))):
            print(f"\n样本 {i + 1} 预测: {predictions[i]}")
            print(f"样本 {i + 1} 真实: {ground_truths[i]}")
            pred_tuples = self.parse_output(predictions[i])
            true_tuples = self.parse_output(ground_truths[i])
            print(f"预测四元组: {pred_tuples}")
            print(f"真实四元组: {true_tuples}")

            # 计算相似度
            if pred_tuples and true_tuples:
                sim = difflib.SequenceMatcher(None, str(pred_tuples[0]), str(true_tuples[0])).ratio()
                print(f"四元组相似度: {sim:.2f}")

            print("-" * 50)

        hard_f1_scores = []
        soft_f1_scores = []
        hard_precisions = []
        hard_recalls = []
        soft_precisions = []
        soft_recalls = []

        # 统计匹配成功的数量
        successful_matches = 0

        # 统计仇恨言论识别准确率
        hate_tp = 0
        hate_fp = 0
        hate_tn = 0
        hate_fn = 0

        for i, (pred, true) in enumerate(zip(predictions, ground_truths)):
            pred_tuples = self.parse_output(pred)
            true_tuples = self.parse_output(true)

            # 硬匹配
            hard_p, hard_r, hard_f1 = self.hard_match(pred_tuples, true_tuples)
            hard_precisions.append(hard_p)
            hard_recalls.append(hard_r)
            hard_f1_scores.append(hard_f1)

            # 软匹配
            soft_p, soft_r, soft_f1 = self.soft_match(pred_tuples, true_tuples)
            soft_precisions.append(soft_p)
            soft_recalls.append(soft_r)
            soft_f1_scores.append(soft_f1)

            # 统计成功匹配的样本
            if soft_f1 > 0:
                successful_matches += 1

            # 统计仇恨言论识别
            if pred_tuples and true_tuples:
                pred_hate = pred_tuples[0][3].lower() == "hate"
                true_hate = true_tuples[0][3].lower() == "hate"

                if pred_hate and true_hate:
                    hate_tp += 1
                elif pred_hate and not true_hate:
                    hate_fp += 1
                elif not pred_hate and not true_hate:
                    hate_tn += 1
                elif not pred_hate and true_hate:
                    hate_fn += 1

            # 详细输出每个样本的评估结果
            if i < 10 or soft_f1 > 0 or hard_f1 > 0:
                print(f"样本 {i + 1} - 硬匹配F1: {hard_f1:.2f}, 软匹配F1: {soft_f1:.2f}")
                if soft_f1 > 0 or hard_f1 > 0:
                    print(f"成功匹配！预测: {pred}")
                    print(f"真实: {true}")

        # 计算仇恨言论识别性能
        hate_precision = hate_tp / (hate_tp + hate_fp) if (hate_tp + hate_fp) > 0 else 0
        hate_recall = hate_tp / (hate_tp + hate_fn) if (hate_tp + hate_fn) > 0 else 0
        hate_f1 = 2 * hate_precision * hate_recall / (hate_precision + hate_recall) if (
                                                                                                   hate_precision + hate_recall) > 0 else 0
        hate_accuracy = (hate_tp + hate_tn) / (hate_tp + hate_tn + hate_fp + hate_fn) if (
                                                                                                     hate_tp + hate_tn + hate_fp + hate_fn) > 0 else 0

        # 总体统计
        print(
            f"\n成功匹配样本数: {successful_matches}/{len(predictions)} ({successful_matches / len(predictions) * 100:.2f}%)")
        print(f"仇恨言论识别准确率: {hate_accuracy:.4f}")

        results = {
            "硬匹配精确率": np.mean(hard_precisions),
            "硬匹配召回率": np.mean(hard_recalls),
            "硬匹配F1": np.mean(hard_f1_scores),
            "软匹配精确率": np.mean(soft_precisions),
            "软匹配召回率": np.mean(soft_recalls),
            "软匹配F1": np.mean(soft_f1_scores),
            "平均F1": (np.mean(hard_f1_scores) + np.mean(soft_f1_scores)) / 2,
            "仇恨言论识别准确率": hate_accuracy,
        }

        return results


def test_on_dataset(detector: ChineseHateSpeechDetector, evaluator: HateSpeechEvaluator, test_file: str) -> Dict:
    """在测试集上进行测试"""
    print(f"正在测试数据集: {test_file}")

    # 读取测试数据
    try:
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        print(f"成功读取测试数据: {len(test_data)} 条")
    except Exception as e:
        print(f"读取测试文件失败: {e}")
        try:
            with open(test_file, 'r', encoding='gbk') as f:
                test_data = json.load(f)
            print(f"使用GBK编码成功读取测试数据: {len(test_data)} 条")
        except Exception as e:
            print(f"所有编码尝试失败: {e}")
            return {'scores': {}, 'predictions': [], 'ground_truths': [], 'num_samples': 0}

    predictions = []
    ground_truths = []
    valid_samples = 0

    print(f"正在处理 {len(test_data)} 个测试样本...")

    for i, item in enumerate(test_data):
        text = item.get('content', '')
        true_label = item.get('output', '')

        if not text.strip():
            print(f"警告: 样本 {i + 1} 没有文本内容")
            continue

        # 处理可能的空输出
        if not true_label.strip():
            print(f"警告: 样本 {i + 1} 没有标签") if i < 10 else None
            true_label = "NULL | NULL | 其他 | Non-hate [END]"

        # 确保输出格式正确
        if " | " not in true_label:
            print(f"警告: 样本 {i + 1} 标签格式不正确: {true_label}") if i < 10 else None
            # 尝试修复格式
            if not true_label.endswith(" [END]"):
                true_label = true_label.strip() + " [END]"
            if " | " not in true_label:
                # 简单修复：假设整个字符串是目标
                parts = true_label.replace(" [END]", "").strip().split()
                if parts:
                    target = parts[0]
                    true_label = f"{target} | NULL | 其他 | Non-hate [END]"
                else:
                    true_label = "NULL | NULL | 其他 | Non-hate [END]"

        # 生成预测
        tuples = detector.extract_tuples(text)
        prediction = detector.format_output(tuples)

        # 确保预测格式一致
        prediction = prediction.strip()
        if not prediction.endswith(" [END]"):
            prediction += " [END]"

        # 确保true_label格式正确
        true_label = true_label.strip()
        if not true_label.endswith(" [END]"):
            true_label += " [END]"

        predictions.append(prediction)
        ground_truths.append(true_label)
        valid_samples += 1

        # 打印前10个样本的详细结果
        if i < 10:
            print(f"\n=== 样本 {i + 1} ===")
            print(f"原文: {text}")
            print(f"真实标签: {true_label}")
            print(f"预测结果: {prediction}")
            print(f"提取的四元组: {tuples}")

        # 每500个样本打印一次进度
        if i % 500 == 499:
            print(f"已处理 {i + 1}/{len(test_data)} 个样本")

    print(f"有效样本数: {valid_samples}/{len(test_data)}")

    # 计算评价指标
    scores = evaluator.evaluate(predictions, ground_truths)

    print(f"\n=== 评价结果 ===")
    for metric, score in scores.items():
        print(f"{metric}: {score:.4f}")

    # 保存所有预测结果到文本文件（仅包含预测结果，每行一个）
    try:
        output_file = "test1_prediction.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            for pred in predictions:
                f.write(f"{pred}\n")
        print(f"\n所有预测结果已保存到文件: {output_file}")
    except Exception as e:
        print(f"保存预测结果失败: {e}")

    return {
        'scores': scores,
        'predictions': predictions,
        'ground_truths': ground_truths,
        'num_samples': valid_samples
    }


def train_and_test_pipeline(train_file: str, test_file: str, model_save_path: str = "hate_speech_model.pkl"):
    """完整的训练和测试流程"""

    print("=== 开始训练 ===")
    # 1. 训练模型
    trainer = ChineseHateSpeechTrainer()

    try:
        training_data = trainer.parse_training_data(train_file)
        if not training_data:
            print("警告: 训练数据为空或解析失败")
            return None

        trainer.extract_features_from_training(training_data)
        trainer.save_model(model_save_path)
    except Exception as e:
        import traceback
        print(f"训练过程出错: {e}")
        traceback.print_exc()
        return None

    print("\n=== 开始测试 ===")
    # 2. 加载训练好的模型进行测试
    detector = ChineseHateSpeechDetector(model_save_path)
    evaluator = HateSpeechEvaluator()

    # 3. 测试第一个测试集
    print(f"\n--- 测试集结果: {test_file} ---")
    try:
        test_results = test_on_dataset(detector, evaluator, test_file)
        return test_results
    except Exception as e:
        import traceback
        print(f"测试集评估出错: {e}")
        traceback.print_exc()
        return None


def main():
    """主函数"""
    import os

    # 设置文件路径
    train_file = "train.json"
    test_file = "test1.json"

    # 检查文件是否存在
    files_to_check = [train_file, test_file]
    all_files_exist = True

    for file_path in files_to_check:
        if not os.path.exists(file_path):
            print(f"警告: 文件不存在 - {file_path}")
            all_files_exist = False

    # 执行训练和测试流程
    if all_files_exist:
        try:
            print("\n=== 执行完整训练和测试流程 ===")
            train_and_test_pipeline(train_file, test_file)
        except Exception as e:
            import traceback
            print(f"执行过程中出错: {e}")
            traceback.print_exc()
    else:
        print("\n文件路径有误，请确保以下文件在当前目录中:")
        print("- train.json: 训练数据")
        print("- test1.json: 测试集")


if __name__ == "__main__":
    main()
